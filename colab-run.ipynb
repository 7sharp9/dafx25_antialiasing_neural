{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1j0s8yUxLcvd"
      },
      "source": [
        "# Anti-Aliasing Neural Network Training on Google Colab\n",
        "\n",
        "This notebook enables GPU-accelerated training of anti-aliasing RNN models.\n",
        "\n",
        "**Expected speedup:** 20-30x (from 32 min/epoch on CPU to ~1-2 min/epoch on T4 GPU)\n",
        "\n",
        "## Setup Overview\n",
        "\n",
        "1. Mount Google Drive for persistent storage\n",
        "2. Clone repository from GitHub\n",
        "3. Install dependencies\n",
        "4. Create symlinks for weights/audio/checkpoints\n",
        "5. Configure wandb for remote monitoring\n",
        "6. Run training\n",
        "\n",
        "## Before Running\n",
        "\n",
        "Ensure your Google Drive has this structure:\n",
        "```\n",
        "Google Drive/\n",
        "└── AA_Neural/\n",
        "    ├── weights/\n",
        "    │   └── NAM/\n",
        "    │       └── Marshall JCM 800 2203/\n",
        "    │           └── JCM800 2203 - P5 B5 M5 T5 MV7 G10 - AZG - 700.nam\n",
        "    ├── audio_data/\n",
        "    │   └── val_input.wav\n",
        "    └── checkpoints/\n",
        "        (created automatically)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLKfssm3Lcvi"
      },
      "source": [
        "## Quick Start\n",
        "\n",
        "**First time setup:**\n",
        "1. Update `REPO_URL` in cell 2 with your GitHub repository URL\n",
        "2. Run all cells in order (Runtime → Run all)\n",
        "3. Authenticate Google Drive when prompted\n",
        "4. Enter wandb API key when prompted\n",
        "\n",
        "**Subsequent runs:**\n",
        "- Just run all cells again\n",
        "- Previous checkpoints remain in Drive\n",
        "\n",
        "**Monitor training:**\n",
        "- Live metrics appear below each cell\n",
        "- Visit wandb.ai dashboard for full visualization\n",
        "- Can close this tab - training continues, checkpoints save to Drive\n",
        "\n",
        "**Troubleshooting:**\n",
        "- If Drive mount fails: Runtime → Reset all runtimes, try again\n",
        "- If out of memory: Reduce batch_size in config.py\n",
        "- If session disconnects: Re-run notebook, checkpoints in Drive are preserved"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oipeHHNrLcvj"
      },
      "source": [
        "## 1. Mount Google Drive\n",
        "\n",
        "This mounts your Google Drive to `/content/drive` for persistent storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdpzfxffLcvj",
        "outputId": "d601d592-acab-42d4-9448-5d34d41dd2be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted\n",
            "✓ Google Drive mounted successfully\n",
            "✓ Found /content/drive/MyDrive/AA_Neural\n",
            "✓ Found /content/drive/MyDrive/AA_Neural/weights\n",
            "✓ Found /content/drive/MyDrive/AA_Neural/audio_data\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Check if Drive is already accessible\n",
        "if os.path.exists('/content/drive/MyDrive'):\n",
        "    print(\"Drive already mounted\")\n",
        "else:\n",
        "    # Clean up /content/drive if it exists but isn't mounted\n",
        "    if os.path.exists('/content/drive'):\n",
        "        import shutil\n",
        "        shutil.rmtree('/content/drive')\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Verify the AA_Neural folder exists\n",
        "drive_base = '/content/drive/MyDrive/AA_Neural'\n",
        "assert os.path.exists(drive_base), f\"ERROR: {drive_base} not found. Create it and upload weights/audio_data first.\"\n",
        "assert os.path.exists(f'{drive_base}/weights'), f\"ERROR: {drive_base}/weights not found.\"\n",
        "assert os.path.exists(f'{drive_base}/audio_data'), f\"ERROR: {drive_base}/audio_data not found.\"\n",
        "\n",
        "print(\"\\u2713 Google Drive mounted successfully\")\n",
        "print(f\"\\u2713 Found {drive_base}\")\n",
        "print(f\"\\u2713 Found {drive_base}/weights\")\n",
        "print(f\"\\u2713 Found {drive_base}/audio_data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Clone Repository\n",
        "\n",
        "Clone the repository from GitHub. Update the URL below with your repo."
      ],
      "metadata": {
        "id": "2DKy4bYOLcvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Configuration\n",
        "REPO_URL = 'https://github.com/7sharp9/dafx25_antialiasing_neural.git'  # UPDATE THIS\n",
        "REPO_DIR = '/content/dafx25_antialiasing_neural'\n",
        "\n",
        "# Remove existing clone if present\n",
        "if os.path.exists(REPO_DIR):\n",
        "    print(f\"Removing existing {REPO_DIR}\")\n",
        "    !rm -rf {REPO_DIR}\n",
        "\n",
        "# Clone repository\n",
        "print(f\"Cloning {REPO_URL}...\")\n",
        "!git clone --recurse-submodules {REPO_URL} {REPO_DIR}\n",
        "\n",
        "# Change to repo directory\n",
        "os.chdir(REPO_DIR)\n",
        "print(f\"\\n\\u2713 Repository cloned to {REPO_DIR}\")\n",
        "print(f\"\\u2713 Current directory: {os.getcwd()}\")\n",
        "\n",
        "# Verify OpenAmp submodule\n",
        "assert os.path.exists('OpenAmp/Open_Amp/amp_model.py'), \"ERROR: OpenAmp submodule not loaded correctly\"\n",
        "print(\"\\u2713 OpenAmp submodule loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XH-k36PKLcvl",
        "outputId": "12a564fe-f01a-410d-8fd2-63ce6994d63d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning https://github.com/7sharp9/dafx25_antialiasing_neural.git...\n",
            "Cloning into '/content/dafx25_antialiasing_neural'...\n",
            "remote: Enumerating objects: 631, done.\u001b[K\n",
            "remote: Counting objects: 100% (345/345), done.\u001b[K\n",
            "remote: Compressing objects: 100% (270/270), done.\u001b[K\n",
            "remote: Total 631 (delta 84), reused 318 (delta 58), pack-reused 286 (from 1)\u001b[K\n",
            "Receiving objects: 100% (631/631), 234.16 MiB | 19.67 MiB/s, done.\n",
            "Resolving deltas: 100% (91/91), done.\n",
            "Submodule 'OpenAmp' (https://github.com/Alec-Wright/OpenAmp.git) registered for path 'OpenAmp'\n",
            "Cloning into '/content/dafx25_antialiasing_neural/OpenAmp'...\n",
            "remote: Enumerating objects: 847, done.        \n",
            "remote: Counting objects: 100% (102/102), done.        \n",
            "remote: Compressing objects: 100% (81/81), done.        \n",
            "remote: Total 847 (delta 41), reused 73 (delta 20), pack-reused 745 (from 1)        \n",
            "Receiving objects: 100% (847/847), 269.83 MiB | 24.24 MiB/s, done.\n",
            "Resolving deltas: 100% (80/80), done.\n",
            "Submodule path 'OpenAmp': checked out 'fb6901912ce5cbe96dc757be4add9dc130e7556d'\n",
            "\n",
            "✓ Repository cloned to /content/dafx25_antialiasing_neural\n",
            "✓ Current directory: /content/dafx25_antialiasing_neural\n",
            "✓ OpenAmp submodule loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Install Dependencies\n",
        "\n",
        "Install required Python packages. Colab already has PyTorch, so we only install additional dependencies."
      ],
      "metadata": {
        "id": "KjvxAZwELcvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install core dependencies\n",
        "!pip install -q pytorch-lightning wandb auraloss neural-amp-modeler librosa\n",
        "\n",
        "# Verify critical imports\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "import wandb\n",
        "\n",
        "print(\"\\u2713 Dependencies installed\")\n",
        "print(f\"\\u2713 PyTorch version: {torch.__version__}\")\n",
        "print(f\"\\u2713 PyTorch Lightning version: {pl.__version__}\")\n",
        "print(f\"\\u2713 CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\\u2713 GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-UhaqyeLcvm",
        "outputId": "560898e5-f3ec-4550-f34f-c96bc0b19fb7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Dependencies installed\n",
            "✓ PyTorch version: 2.9.0+cu126\n",
            "✓ PyTorch Lightning version: 2.6.0\n",
            "✓ CUDA available: True\n",
            "✓ GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgpW1QAzLcvm"
      },
      "source": [
        "## 4. Link Drive Storage\n",
        "\n",
        "Create symlinks so the training script reads weights/audio from Drive and saves checkpoints to Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "aEfUHG5oLcvn",
        "outputId": "f51e7640-5ea7-4d8b-c1e9-2b059fb3b8a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Linked weights -> /content/drive/MyDrive/AA_Neural/weights\n",
            "✓ Linked audio_data -> /content/drive/MyDrive/AA_Neural/audio_data\n",
            "✓ Linked lightning_logs -> /content/drive/MyDrive/AA_Neural/checkpoints\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ERROR: weights/NAM/Marshall JCM 800 2203/JCM800 2203 - P5 B5 M5 T5 MV7 G10 - AZG - 700.nam not found. Check that Drive has weights/NAM/Marshall JCM 800 2203/ directory.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-599421612.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Verify critical files exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mtest_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'weights/NAM/Marshall JCM 800 2203/JCM800 2203 - P5 B5 M5 T5 MV7 G10 - AZG - 700.nam'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"ERROR: {test_file} not found. Check that Drive has weights/NAM/Marshall JCM 800 2203/ directory.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\u2713 Verified model file accessible: {test_file}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: ERROR: weights/NAM/Marshall JCM 800 2203/JCM800 2203 - P5 B5 M5 T5 MV7 G10 - AZG - 700.nam not found. Check that Drive has weights/NAM/Marshall JCM 800 2203/ directory."
          ]
        }
      ],
      "source": [
        "# @title\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "drive_base = '/content/drive/MyDrive/AA_Neural'\n",
        "repo_dir = '/content/dafx25_antialiasing_neural'\n",
        "\n",
        "# Ensure we're in the repo directory\n",
        "os.chdir(repo_dir)\n",
        "\n",
        "# Create symlinks for persistent storage\n",
        "symlinks = [\n",
        "    ('weights', f'{drive_base}/weights'),\n",
        "    ('audio_data', f'{drive_base}/audio_data'),\n",
        "    ('lightning_logs', f'{drive_base}/checkpoints'),  # Redirect PL checkpoints\n",
        "]\n",
        "\n",
        "for link_name, target in symlinks:\n",
        "    link_path = f'{repo_dir}/{link_name}'\n",
        "\n",
        "    # Remove existing file/dir/symlink\n",
        "    if os.path.islink(link_path):\n",
        "        os.unlink(link_path)\n",
        "    elif os.path.isdir(link_path):\n",
        "        shutil.rmtree(link_path)\n",
        "    elif os.path.exists(link_path):\n",
        "        os.remove(link_path)\n",
        "\n",
        "    # Create target directory if it doesn't exist\n",
        "    os.makedirs(target, exist_ok=True)\n",
        "\n",
        "    # Create symlink\n",
        "    os.symlink(target, link_path)\n",
        "    print(f\"\\u2713 Linked {link_name} -> {target}\")\n",
        "\n",
        "# Verify critical files exist\n",
        "test_file = 'weights/NAM/Marshall JCM 800 2203/JCM800 2203 - P5 B5 M5 T5 MV7 G10 - AZG - 700.nam'\n",
        "assert os.path.exists(test_file), f\"ERROR: {test_file} not found. Check that Drive has weights/NAM/Marshall JCM 800 2203/ directory.\"\n",
        "print(f\"\\u2713 Verified model file accessible: {test_file}\")\n",
        "\n",
        "test_audio = 'audio_data/val_input.wav'\n",
        "assert os.path.exists(test_audio), f\"ERROR: {test_audio} not found. Check that Drive has audio_data/val_input.wav file.\"\n",
        "print(f\"\\u2713 Verified audio file accessible: {test_audio}\")\n",
        "\n",
        "print(f\"\\u2713 Checkpoints will save to {drive_base}/checkpoints\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Configure wandb\n",
        "\n",
        "Login to Weights & Biases for experiment tracking. Get your API key from https://wandb.ai/authorize"
      ],
      "metadata": {
        "id": "U4681vxXLcvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "# Login to wandb\n",
        "# Option 1: Interactive login (will prompt for API key)\n",
        "wandb.login()\n",
        "\n",
        "# Option 2: Programmatic login (uncomment and add your key)\n",
        "# wandb.login(key='your-api-key-here')\n",
        "\n",
        "print(\"\\u2713 wandb configured\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezldLPfOLcvn",
        "outputId": "9acd5fb8-d01e-486a-f1f8-8869dc3607ba"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m7sharp9\u001b[0m (\u001b[33m7sharp9-moirae-software-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ wandb configured\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Training Configuration\n",
        "\n",
        "Set training parameters. Modify these as needed."
      ],
      "metadata": {
        "id": "8o7AgSBtLcvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training configuration\n",
        "CONFIG_IDX = 3          # Model config (3 = JCM800 NAM model)\n",
        "MAX_EPOCHS = 100        # Number of epochs to train\n",
        "USE_WANDB = True        # Enable wandb logging\n",
        "\n",
        "print(f\"Training configuration:\")\n",
        "print(f\"  Config index: {CONFIG_IDX}\")\n",
        "print(f\"  Max epochs: {MAX_EPOCHS}\")\n",
        "print(f\"  wandb logging: {USE_WANDB}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8X6zQcf9Lcvp",
        "outputId": "027f0385-70ac-48dd-c552-098891b495ce"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training configuration:\n",
            "  Config index: 3\n",
            "  Max epochs: 100\n",
            "  wandb logging: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Run Training\n",
        "\n",
        "Execute the training script. Progress will stream to this notebook.\n",
        "\n",
        "**Expected time:** ~2-3 hours for 100 epochs on T4 GPU\n",
        "\n",
        "**Monitoring:**\n",
        "- Live metrics visible in this notebook\n",
        "- Full dashboard at wandb.ai\n",
        "- Checkpoints auto-save to Drive"
      ],
      "metadata": {
        "id": "08MLM5LFLcvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Ensure we're in the repo directory\n",
        "os.chdir('/content/dafx25_antialiasing_neural')\n",
        "\n",
        "# Build command\n",
        "cmd_parts = [\n",
        "    'python train.py',\n",
        "    f'--config {CONFIG_IDX}',\n",
        "    f'--max_epochs {MAX_EPOCHS}',\n",
        "]\n",
        "\n",
        "if USE_WANDB:\n",
        "    cmd_parts.append('--wandb')\n",
        "else:\n",
        "    cmd_parts.append('--no-wandb')\n",
        "\n",
        "cmd = ' '.join(cmd_parts)\n",
        "\n",
        "print(f\"Executing: {cmd}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Run training (this will take a while)\n",
        "!{cmd}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWFdBXqkLcvq",
        "outputId": "ee33ceb1-8598-468a-b5b4-05a7e0d0f7ca"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing: python train.py --config 3 --max_epochs 100 --wandb\n",
            "================================================================================\n",
            "Configuration: JCM800\n",
            "Model: weights/NAM/Marshall JCM 800 2203/JCM800 2203 - P5 B5 M5 T5 MV7 G10 - AZG - 700.nam\n",
            "CPU cores: 2\n",
            "Seed set to 42\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/dafx25_antialiasing_neural/train.py\", line 657, in <module>\n",
            "    main()\n",
            "  File \"/content/dafx25_antialiasing_neural/train.py\", line 586, in main\n",
            "    model = AARNN(conf)\n",
            "            ^^^^^^^^^^^\n",
            "  File \"/content/dafx25_antialiasing_neural/train.py\", line 57, in __init__\n",
            "    self.model = AmpModel(conf['model_json'], conf['model_name'])\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/dafx25_antialiasing_neural/OpenAmp/Open_Amp/amp_model.py\", line 16, in __init__\n",
            "    self.model, self.framework, self.model_class, self.cond = Get_Open_Amp_Model(model_file)\n",
            "                                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/dafx25_antialiasing_neural/OpenAmp/Open_Amp/amp_model.py\", line 132, in Get_Open_Amp_Model\n",
            "    with open(filename, 'r') as f:\n",
            "         ^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'weights/NAM/Marshall JCM 800 2203/JCM800 2203 - P5 B5 M5 T5 MV7 G10 - AZG - 700.nam'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Post-Training\n",
        "\n",
        "After training completes, checkpoints are in Google Drive."
      ],
      "metadata": {
        "id": "PMbr3yEPLcvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List saved checkpoints\n",
        "import os\n",
        "import glob\n",
        "\n",
        "checkpoints_base = '/content/drive/MyDrive/AA_Neural/checkpoints'\n",
        "\n",
        "# Find all checkpoint versions\n",
        "versions = sorted(glob.glob(f'{checkpoints_base}/version_*'))\n",
        "\n",
        "if versions:\n",
        "    latest_version = versions[-1]\n",
        "    print(f\"Found {len(versions)} training run(s)\")\n",
        "    print(f\"\\nLatest run: {latest_version}\")\n",
        "\n",
        "    # List checkpoints in latest run\n",
        "    checkpoints = glob.glob(f'{latest_version}/checkpoints/*.ckpt')\n",
        "    if checkpoints:\n",
        "        print(f\"\\nCheckpoints ({len(checkpoints)}):\")\n",
        "        for ckpt in sorted(checkpoints):\n",
        "            size_mb = os.path.getsize(ckpt) / (1024*1024)\n",
        "            print(f\"  {os.path.basename(ckpt)} ({size_mb:.1f} MB)\")\n",
        "\n",
        "    # Check for best model export\n",
        "    best_export = f'{latest_version}/best_export'\n",
        "    if os.path.exists(best_export):\n",
        "        print(f\"\\n✓ Best model exported to: {best_export}\")\n",
        "        exported_files = glob.glob(f'{best_export}/*')\n",
        "        for f in exported_files:\n",
        "            print(f\"  {os.path.basename(f)}\")\n",
        "else:\n",
        "    print(\"No checkpoints found. Training may not have completed.\")\n",
        "\n",
        "print(f\"\\nAll results saved to Google Drive: {checkpoints_base}\")"
      ],
      "metadata": {
        "id": "JGrkagOHLcvq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}