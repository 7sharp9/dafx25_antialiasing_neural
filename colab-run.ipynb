{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1j0s8yUxLcvd"
   },
   "source": "# Anti-Aliasing Neural Network Training on Google Colab\n\nThis notebook enables GPU-accelerated training of anti-aliasing RNN models.\n\n**Expected speedup:** 20-30x (from 32 min/epoch on CPU to ~1-2 min/epoch on T4 GPU)\n\n**Performance optimizations:**\n- GPU acceleration (automatic)\n- Cached dataset generation (optional, recommended for 20-30% additional speedup)\n\n## Setup Overview\n\n1. Mount Google Drive for persistent storage\n2. Clone repository from GitHub\n3. Install dependencies\n4. Create symlinks for weights/audio/checkpoints\n5. Configure wandb for remote monitoring\n6. Generate cached dataset (optional, recommended)\n7. Run training\n\n## Before Running\n\nEnsure your Google Drive has this structure:\n```\nGoogle Drive/\n\u2514\u2500\u2500 AA_Neural/\n    \u251c\u2500\u2500 weights/\n    \u2502   \u2514\u2500\u2500 NAM/\n    \u2502       \u2514\u2500\u2500 Marshall JCM 800 2203/\n    \u2502           \u2514\u2500\u2500 JCM800 2203 - P5 B5 M5 T5 MV7 G10 - AZG - 700.nam\n    \u251c\u2500\u2500 audio_data/\n    \u2502   \u2514\u2500\u2500 val_input.wav\n    \u251c\u2500\u2500 checkpoints/\n    \u2502   (created automatically)\n    \u2514\u2500\u2500 cached_train.pt & cached_val.pt\n        (generated automatically if using cached data)\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLKfssm3Lcvi"
   },
   "source": [
    "## Quick Start\n",
    "\n",
    "**First time setup:**\n",
    "1. Update `REPO_URL` in cell 2 with your GitHub repository URL\n",
    "2. Run all cells in order (Runtime \u2192 Run all)\n",
    "3. Authenticate Google Drive when prompted\n",
    "4. Enter wandb API key when prompted\n",
    "\n",
    "**Subsequent runs:**\n",
    "- Just run all cells again\n",
    "- Previous checkpoints remain in Drive\n",
    "\n",
    "**Monitor training:**\n",
    "- Live metrics appear below each cell\n",
    "- Visit wandb.ai dashboard for full visualization\n",
    "- Can close this tab - training continues, checkpoints save to Drive\n",
    "\n",
    "**Troubleshooting:**\n",
    "- If Drive mount fails: Runtime \u2192 Reset all runtimes, try again\n",
    "- If out of memory: Reduce batch_size in config.py\n",
    "- If session disconnects: Re-run notebook, checkpoints in Drive are preserved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49c0737",
   "metadata": {},
   "source": [
    "## 1. Mount Google Drive\n",
    "\n",
    "This mounts your Google Drive to `/content/drive` for persistent storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CdpzfxffLcvj",
    "outputId": "4cbe6157-2108-4b1d-f83d-eb6c26efcba4"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Check if Drive is already accessible\n",
    "if os.path.exists('/content/drive/MyDrive'):\n",
    "    print(\"Drive already mounted\")\n",
    "else:\n",
    "    # Clean up /content/drive if it exists but isn't mounted\n",
    "    if os.path.exists('/content/drive'):\n",
    "        import shutil\n",
    "        shutil.rmtree('/content/drive')\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "# Verify the AA_Neural folder exists\n",
    "drive_base = '/content/drive/MyDrive/AA_Neural'\n",
    "assert os.path.exists(drive_base), f\"ERROR: {drive_base} not found. Create it and upload weights/audio_data first.\"\n",
    "assert os.path.exists(f'{drive_base}/weights'), f\"ERROR: {drive_base}/weights not found.\"\n",
    "assert os.path.exists(f'{drive_base}/audio_data'), f\"ERROR: {drive_base}/audio_data not found.\"\n",
    "\n",
    "print(\"\\u2713 Google Drive mounted successfully\")\n",
    "print(f\"\\u2713 Found {drive_base}\")\n",
    "print(f\"\\u2713 Found {drive_base}/weights\")\n",
    "print(f\"\\u2713 Found {drive_base}/audio_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DKy4bYOLcvl"
   },
   "source": [
    "## 2. Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XH-k36PKLcvl",
    "outputId": "9ed1914f-8732-4eb0-caab-d092944bfcb8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Configuration\n",
    "REPO_URL = 'https://github.com/7sharp9/dafx25_antialiasing_neural.git'\n",
    "REPO_DIR = '/content/dafx25_antialiasing_neural'\n",
    "\n",
    "# Remove existing clone if present\n",
    "if os.path.exists(REPO_DIR):\n",
    "    print(f\"Removing existing {REPO_DIR}\")\n",
    "    !rm -rf {REPO_DIR}\n",
    "\n",
    "# Clone repository\n",
    "print(f\"Cloning {REPO_URL}...\")\n",
    "!git clone --recurse-submodules {REPO_URL} {REPO_DIR}\n",
    "\n",
    "# Change to repo directory\n",
    "os.chdir(REPO_DIR)\n",
    "print(f\"\\n\\u2713 Repository cloned to {REPO_DIR}\")\n",
    "print(f\"\\u2713 Current directory: {os.getcwd()}\")\n",
    "\n",
    "# Verify OpenAmp submodule\n",
    "assert os.path.exists('OpenAmp/Open_Amp/amp_model.py'), \"ERROR: OpenAmp submodule not loaded correctly\"\n",
    "print(\"\\u2713 OpenAmp submodule loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjvxAZwELcvm"
   },
   "source": [
    "## 3. Install Dependencies\n",
    "\n",
    "Install required Python packages. Colab already has PyTorch, so we only install additional dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q-UhaqyeLcvm",
    "outputId": "e56c461e-a5f0-48f3-89ef-5f8d3cdfc872"
   },
   "outputs": [],
   "source": [
    "# Install core dependencies\n",
    "!pip install -q pytorch-lightning wandb auraloss neural-amp-modeler librosa torchcodec\n",
    "\n",
    "# Verify critical imports\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import wandb\n",
    "\n",
    "print(\"\\u2713 Dependencies installed\")\n",
    "print(f\"\\u2713 PyTorch version: {torch.__version__}\")\n",
    "print(f\"\\u2713 PyTorch Lightning version: {pl.__version__}\")\n",
    "print(f\"\\u2713 CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\u2713 GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HgpW1QAzLcvm"
   },
   "source": [
    "## 4. Link Drive Storage\n",
    "\n",
    "Create symlinks so the training script reads weights/audio from Drive and saves checkpoints to Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aEfUHG5oLcvn",
    "outputId": "c690659c-425c-4744-bf9b-0d8c56c612bd"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "drive_base = '/content/drive/MyDrive/AA_Neural'\n",
    "repo_dir = '/content/dafx25_antialiasing_neural'\n",
    "\n",
    "# Ensure we're in the repo directory\n",
    "os.chdir(repo_dir)\n",
    "\n",
    "# Create symlinks for persistent storage\n",
    "symlinks = [\n",
    "    ('weights', f'{drive_base}/weights'),\n",
    "    ('audio_data', f'{drive_base}/audio_data'),\n",
    "    ('lightning_logs', f'{drive_base}/checkpoints'),  # Redirect PL checkpoints\n",
    "]\n",
    "\n",
    "for link_name, target in symlinks:\n",
    "    link_path = f'{repo_dir}/{link_name}'\n",
    "\n",
    "    # Remove existing file/dir/symlink\n",
    "    if os.path.islink(link_path):\n",
    "        os.unlink(link_path)\n",
    "    elif os.path.isdir(link_path):\n",
    "        shutil.rmtree(link_path)\n",
    "    elif os.path.exists(link_path):\n",
    "        os.remove(link_path)\n",
    "\n",
    "    # Create target directory if it doesn't exist\n",
    "    os.makedirs(target, exist_ok=True)\n",
    "\n",
    "    # Create symlink\n",
    "    os.symlink(target, link_path)\n",
    "    print(f\"\\u2713 Linked {link_name} -> {target}\")\n",
    "\n",
    "# Verify critical files exist\n",
    "test_file = 'weights/NAM/Marshall JCM 800 2203/JCM800 2203 - P5 B5 M5 T5 MV7 G10 - AZG - 700.nam'\n",
    "assert os.path.exists(test_file), f\"ERROR: {test_file} not found. Check that Drive has weights/NAM/Marshall JCM 800 2203/ directory.\"\n",
    "print(f\"\\u2713 Verified model file accessible: {test_file}\")\n",
    "\n",
    "test_audio = 'audio_data/val_input.wav'\n",
    "assert os.path.exists(test_audio), f\"ERROR: {test_audio} not found. Check that Drive has audio_data/val_input.wav file.\"\n",
    "print(f\"\\u2713 Verified audio file accessible: {test_audio}\")\n",
    "\n",
    "print(f\"\\u2713 Checkpoints will save to {drive_base}/checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4681vxXLcvn"
   },
   "source": [
    "## 5. Configure wandb\n",
    "\n",
    "Login to Weights & Biases for experiment tracking. Get your API key from https://wandb.ai/authorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ezldLPfOLcvn",
    "outputId": "b72ae3fe-6bd4-4f31-e9fe-481c4c2d612e"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Login to wandb\n",
    "# Option 1: Interactive login (will prompt for API key)\n",
    "wandb.login()\n",
    "\n",
    "# Option 2: Programmatic login (uncomment and add your key)\n",
    "# wandb.login(key='your-api-key-here')\n",
    "\n",
    "print(\"\\u2713 wandb configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8o7AgSBtLcvo"
   },
   "source": [
    "## 6. Training Configuration\n",
    "\n",
    "Set training parameters. Modify these as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8X6zQcf9Lcvp",
    "outputId": "5c81aa76-9f4d-4448-db32-c91fc3dd6f9f"
   },
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "CONFIG_IDX = 3          # Model config (3 = JCM800 NAM model)\n",
    "MAX_EPOCHS = 100        # Number of epochs to train\n",
    "USE_WANDB = True        # Enable wandb logging\n",
    "LOG_EVERY_N_STEPS = 10  # Log every 10 steps (adjust as needed)\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Config index: {CONFIG_IDX}\")\n",
    "print(f\"  Max epochs: {MAX_EPOCHS}\")\n",
    "print(f\"  wandb logging: {USE_WANDB}\")\n",
    "print(f\"  Log every N steps: {LOG_EVERY_N_STEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5. Optional: Generate Cached Dataset (Recommended)\n",
    "\n",
    "**Performance optimization:** Pre-compute teacher outputs to eliminate inference overhead during training.\n",
    "\n",
    "**Benefits:**\n",
    "- 20-30% faster training\n",
    "- Consistent results across runs\n",
    "- Run once, use for all experiments\n",
    "\n",
    "**First time:** Run this cell to generate cached data (~5-10 minutes)\n",
    "**Subsequent runs:** Skip this cell if cached data already exists"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "# Configuration\n",
    "drive_base = '/content/drive/MyDrive/AA_Neural'\n",
    "cached_train_path = f'{drive_base}/cached_train.pt'\n",
    "cached_val_path = f'{drive_base}/cached_val.pt'\n",
    "USE_CACHED_DATA = True  # Set to False to disable cached data\n",
    "\n",
    "# Check if cached data already exists\n",
    "if os.path.exists(cached_train_path) and os.path.exists(cached_val_path):\n",
    "    train_size = os.path.getsize(cached_train_path) / (1024*1024)\n",
    "    val_size = os.path.getsize(cached_val_path) / (1024*1024)\n",
    "    print(f\"\u2713 Cached data already exists:\")\n",
    "    print(f\"  Training: {cached_train_path} ({train_size:.1f} MB)\")\n",
    "    print(f\"  Validation: {cached_val_path} ({val_size:.1f} MB)\")\n",
    "    print(f\"\\nUsing existing cached data. To regenerate, delete these files from Drive.\")\n",
    "else:\n",
    "    print(\"Generating cached dataset...\")\n",
    "    print(\"This will take 5-10 minutes but only needs to be done once.\\n\")\n",
    "    \n",
    "    # Change to repo directory\n",
    "    os.chdir('/content/dafx25_antialiasing_neural')\n",
    "    \n",
    "    # Generate cached data\n",
    "    !python generate_cached_dataset.py \\\n",
    "        --config {CONFIG_IDX} \\\n",
    "        --output_train {cached_train_path} \\\n",
    "        --output_val {cached_val_path}\n",
    "    \n",
    "    # Verify generation succeeded\n",
    "    if os.path.exists(cached_train_path) and os.path.exists(cached_val_path):\n",
    "        train_size = os.path.getsize(cached_train_path) / (1024*1024)\n",
    "        val_size = os.path.getsize(cached_val_path) / (1024*1024)\n",
    "        print(f\"\\n\u2713 Cached data generated successfully:\")\n",
    "        print(f\"  Training: {cached_train_path} ({train_size:.1f} MB)\")\n",
    "        print(f\"  Validation: {cached_val_path} ({val_size:.1f} MB)\")\n",
    "        print(f\"\\n\u2713 Cached data saved to Google Drive - will be reused for future runs\")\n",
    "    else:\n",
    "        print(\"\\n\u26a0 WARNING: Cached data generation may have failed\")\n",
    "        print(\"Training will proceed with on-the-fly generation (slower)\")\n",
    "        USE_CACHED_DATA = False\n",
    "\n",
    "print(f\"\\nCached data enabled: {USE_CACHED_DATA}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08MLM5LFLcvp"
   },
   "source": [
    "## 7. Run Training\n",
    "\n",
    "Execute the training script. Progress will stream to this notebook.\n",
    "\n",
    "**Expected time:** \n",
    "- Without cached data: ~2-3 hours for 100 epochs on T4 GPU\n",
    "- With cached data: ~1.5-2.5 hours for 100 epochs on T4 GPU\n",
    "\n",
    "**Monitoring:**\n",
    "- Live metrics visible in this notebook\n",
    "- Full dashboard at wandb.ai\n",
    "- Checkpoints auto-save to Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZWFdBXqkLcvq",
    "outputId": "a6070cb1-36b3-4556-ba71-543e9800be64"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Ensure we're in the repo directory\n",
    "os.chdir('/content/dafx25_antialiasing_neural')\n",
    "\n",
    "# Check if cached data variables exist (from section 6.5)\n",
    "try:\n",
    "    USE_CACHED_DATA\n",
    "    cached_train_path\n",
    "    cached_val_path\n",
    "except NameError:\n",
    "    # If cell 6.5 was skipped, default to no cached data\n",
    "    USE_CACHED_DATA = False\n",
    "    print(\"\u26a0 Cached data variables not found - using on-the-fly generation\")\n",
    "    print(\"  (Run section 6.5 to enable cached data for faster training)\\n\")\n",
    "\n",
    "# Build command\n",
    "cmd_parts = [\n",
    "    'python train.py',\n",
    "    f'--config {CONFIG_IDX}',\n",
    "    f'--max_epochs {MAX_EPOCHS}',\n",
    "]\n",
    "\n",
    "if USE_WANDB:\n",
    "    cmd_parts.append('--wandb')\n",
    "else:\n",
    "    cmd_parts.append('--no-wandb')\n",
    "\n",
    "# Add cached data arguments if enabled\n",
    "if USE_CACHED_DATA:\n",
    "    cmd_parts.extend([\n",
    "        '--use_cached_data',\n",
    "        f'--cached_train_path {cached_train_path}',\n",
    "        f'--cached_val_path {cached_val_path}',\n",
    "    ])\n",
    "    print(\"\u2713 Using cached dataset for faster training\\n\")\n",
    "\n",
    "cmd = ' '.join(cmd_parts)\n",
    "\n",
    "print(f\"Executing: {cmd}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Run training (this will take a while)\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMbr3yEPLcvq"
   },
   "source": [
    "## 8. Post-Training\n",
    "\n",
    "After training completes, checkpoints are in Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JGrkagOHLcvq"
   },
   "outputs": [],
   "source": [
    "# List saved checkpoints\n",
    "import os\n",
    "import glob\n",
    "\n",
    "checkpoints_base = '/content/drive/MyDrive/AA_Neural/checkpoints'\n",
    "\n",
    "# Find all checkpoint versions\n",
    "versions = sorted(glob.glob(f'{checkpoints_base}/version_*'))\n",
    "\n",
    "if versions:\n",
    "    latest_version = versions[-1]\n",
    "    print(f\"Found {len(versions)} training run(s)\")\n",
    "    print(f\"\\nLatest run: {latest_version}\")\n",
    "\n",
    "    # List checkpoints in latest run\n",
    "    checkpoints = glob.glob(f'{latest_version}/checkpoints/*.ckpt')\n",
    "    if checkpoints:\n",
    "        print(f\"\\nCheckpoints ({len(checkpoints)}):\")\n",
    "        for ckpt in sorted(checkpoints):\n",
    "            size_mb = os.path.getsize(ckpt) / (1024*1024)\n",
    "            print(f\"  {os.path.basename(ckpt)} ({size_mb:.1f} MB)\")\n",
    "\n",
    "    # Check for best model export\n",
    "    best_export = f'{latest_version}/best_export'\n",
    "    if os.path.exists(best_export):\n",
    "        print(f\"\\n\u2713 Best model exported to: {best_export}\")\n",
    "        exported_files = glob.glob(f'{best_export}/*')\n",
    "        for f in exported_files:\n",
    "            print(f\"  {os.path.basename(f)}\")\n",
    "else:\n",
    "    print(\"No checkpoints found. Training may not have completed.\")\n",
    "\n",
    "print(f\"\\nAll results saved to Google Drive: {checkpoints_base}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}